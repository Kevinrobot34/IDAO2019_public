{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tables\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import repeat\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas(desc=\"\")\n",
    "import sys, gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import model_tuner\n",
    "import scoring, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = dict(zip(utils.FULL_COLUMNS + utils.TRAIN_COLUMNS, repeat(np.float32)))\n",
    "TYPES['id'] = np.uint64\n",
    "TYPES['label'] = np.uint8\n",
    "TEST_TYPES = dict(zip(utils.FULL_COLUMNS, repeat(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCHEDHIT_MISSING_COL = ['MatchedHit_X[2]', 'MatchedHit_X[3]', 'MatchedHit_Y[2]', 'MatchedHit_Y[3]', \n",
    "                           'MatchedHit_Z[2]', 'MatchedHit_Z[3]', 'MatchedHit_DX[2]', 'MatchedHit_DX[3]', \n",
    "                           'MatchedHit_DY[2]', 'MatchedHit_DY[3]', 'MatchedHit_DZ[2]', 'MatchedHit_DZ[3]']\n",
    "CLOSESTHIT_MISSING_COL = ['closest_x_per_station[2]', 'closest_x_per_station[3]', \n",
    "                    'closest_y_per_station[2]', 'closest_y_per_station[3]',\n",
    "                    'closest_T_per_station[2]', 'closest_T_per_station[3]',\n",
    "                    'closest_z_per_station[2]', 'closest_z_per_station[3]',\n",
    "                    'closest_dx_per_station[2]', 'closest_dx_per_station[3]',\n",
    "                    'closest_dy_per_station[2]', 'closest_dy_per_station[3]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xdata(dataDF, additional_feature=2, parse=True, pangle = True, drop_FOI = True):\n",
    "    # FOI related feature\n",
    "    if additional_feature == 0:\n",
    "        new_features = \\\n",
    "            dataDF[utils.FOI_COLUMNS].progress_apply(utils.parse_row, result_type=\"expand\", axis=1)\n",
    "    elif additional_feature == 1:\n",
    "        new_features = \\\n",
    "            dataDF[['Lextra_{}[{}]'.format(x, i) for x in ['X', 'Y'] for i in range(4)] + utils.FOI_COLUMNS].progress_apply(utils.find_closest_hit_per_station, parse=parse, result_type=\"expand\", axis=1)\n",
    "        new_features.columns = utils.CLOSEST_HIT_FEATURE_COLUMNS\n",
    "    elif additional_feature == 2:\n",
    "        new_features = \\\n",
    "            dataDF[['Lextra_{}[{}]'.format(x, i) for x in ['X', 'Y'] for i in range(4)] + utils.FOI_COLUMNS].progress_apply(utils.add_foi_features, parse=parse, result_type=\"expand\", axis=1)\n",
    "        new_features.columns = utils.CLOSEST_HIT_FEATURE_COLUMNS + utils.AVERAGE_HIT_COLUMNS + utils.HIT_COUNT_COLUMNS\n",
    "    print(\"additional features : DONE\")\n",
    "    \n",
    "    # drop\n",
    "    if drop_FOI:\n",
    "        dataDF = pd.concat([dataDF[utils.SIMPLE_FEATURE_COLUMNS], new_features], axis=1)\n",
    "    else:\n",
    "        dataDF = pd.concat([dataDF, new_features], axis=1)\n",
    "    print(\"drop : DONE\")\n",
    "    \n",
    "    del new_features\n",
    "    \n",
    "    # imputation\n",
    "    dataDF = replace_nan(dataDF)\n",
    "    dataDF = missing_value_imputation_basic(dataDF)\n",
    "    print(\"data imputation : DONE\")\n",
    "    \n",
    "    # original feature\n",
    "    if pangle:\n",
    "        dataDF[\"Pangle\"] = dataDF[\"PT\"] / dataDF[\"P\"]\n",
    "        dataDF[\"Pangle\"] = dataDF[\"Pangle\"].apply(lambda x: np.arcsin(x))\n",
    "    print(\"original features : DONE\")\n",
    "    \n",
    "    for station in range(2):\n",
    "        delta = pd.DataFrame(index = dataDF.index)\n",
    "        for axis in ['X', 'Y', 'Z']:\n",
    "            delta[axis+'1'] = dataDF['MatchedHit_{}[{}]'.format(axis, station + 1)] - dataDF['MatchedHit_{}[{}]'.format(axis, station)]\n",
    "            delta[axis+'2'] = dataDF['MatchedHit_{}[{}]'.format(axis, station + 2)] - dataDF['MatchedHit_{}[{}]'.format(axis, station+1)]\n",
    "        delta = delta[['X1','Y1','Z1','X2','Y2','Z2']]\n",
    "        dataDF['MAngle[{}]'.format(station)] = delta.progress_apply(lambda x: angle(x.values), axis=1)\n",
    "    del delta\n",
    "    \n",
    "    gc.collect()\n",
    "    return dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_imputation_basic(dataDF):\n",
    "    imputation_dict = {\n",
    "        'MatchedHit_X[2]':'Lextra_X[2]', \n",
    "        'MatchedHit_X[3]':'Lextra_X[3]', \n",
    "        'MatchedHit_Y[2]':'Lextra_Y[2]',\n",
    "        'MatchedHit_Y[3]':'Lextra_Y[3]',\n",
    "        'closest_x_per_station[2]':'Lextra_X[2]',\n",
    "        'closest_x_per_station[3]':'Lextra_X[3]', \n",
    "        'closest_y_per_station[2]':'Lextra_Y[2]', \n",
    "        'closest_y_per_station[3]':'Lextra_Y[3]',\n",
    "        'closest_T_per_station[2]':'MatchedHit_T[2]',\n",
    "        'closest_T_per_station[3]':'MatchedHit_T[3]'\n",
    "    }\n",
    "    self_imputation_cols = ['MatchedHit_Z[2]', 'MatchedHit_Z[3]']\n",
    "    for station in [2,3]: # Assume the ratio of DX(n+1)/DX(n) is the same\n",
    "        MatchedHit_R = {}\n",
    "        closesthit_R = {}\n",
    "        for axis in ['X', 'Y', 'Z']:\n",
    "            MatchedHit_R[axis] = \\\n",
    "            dataDF['MatchedHit_D{}[{}]'.format(axis,station)].dropna().median() / dataDF['MatchedHit_D{}[{}]'.format(axis,station-1)].dropna().median()\n",
    "        for axis in ['x', 'y']:\n",
    "            closesthit_R[axis] = \\\n",
    "            dataDF['closest_d{}_per_station[{}]'.format(axis,station)].dropna().median() / dataDF['closest_d{}_per_station[{}]'.format(axis,station-1)].dropna().median()\n",
    "        for axis in ['X', 'Y', 'Z']:\n",
    "            col = 'MatchedHit_D{}[{}]'.format(axis, station)\n",
    "            col_prev = 'MatchedHit_D{}[{}]'.format(axis, station-1)\n",
    "            ind_null = dataDF[col].isnull()\n",
    "            dataDF.loc[ind_null, col] = dataDF.loc[ind_null, col_prev] * MatchedHit_R[axis]\n",
    "        for axis in ['x', 'y']:\n",
    "            col = 'closest_d{}_per_station[{}]'.format(axis,station)\n",
    "            col_prev = 'closest_d{}_per_station[{}]'.format(axis,station-1)\n",
    "            ind_null = dataDF[col].isnull()\n",
    "            dataDF.loc[ind_null, col] = dataDF.loc[ind_null, col_prev] * closesthit_R[axis]\n",
    "    \n",
    "    for mcol in imputation_dict.keys():\n",
    "        ind_null = dataDF[mcol].isnull()\n",
    "        dataDF.loc[ind_null, mcol] = dataDF.loc[ind_null, imputation_dict[mcol]]\n",
    "    \n",
    "    for col in ['MatchedHit_Z[2]', 'MatchedHit_Z[3]', 'closest_z_per_station[2]', 'closest_z_per_station[3]']:\n",
    "        ind_null = dataDF[col].isnull()\n",
    "        dataDF.loc[ind_null, col] = dataDF.loc[ind_null.apply(lambda x: not x), col].mean()\n",
    "    \n",
    "    return dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(dataDF):\n",
    "    for col in tqdm(MATCHEDHIT_MISSING_COL):\n",
    "        dataDF[col] = dataDF[col].replace(-9999, np.nan)\n",
    "#     for col in tqdm(CLOSESTHIT_MISSING_COL):\n",
    "#         dataDF[col] = dataDF[col].replace(utils.EMPTY_FILLER, np.nan) \n",
    "    return dataDF\n",
    "        \n",
    "def missing_value_imputation(dataDF, essential=False, ratio_f=True, substitution_f=True, mean_f=True, angle_f=True, average_xy_f=True):\n",
    "    imputation_dict = {\n",
    "        'MatchedHit_X[2]':'Lextra_X[2]', \n",
    "        'MatchedHit_X[3]':'Lextra_X[3]', \n",
    "        'MatchedHit_Y[2]':'Lextra_Y[2]',\n",
    "        'MatchedHit_Y[3]':'Lextra_Y[3]',\n",
    "        'closest_T_per_station[2]':'MatchedHit_T[2]',\n",
    "        'closest_T_per_station[3]':'MatchedHit_T[3]'\n",
    "    }\n",
    "    if ratio_f == True:\n",
    "        for station in tqdm([2,3], desc='ratio imputation'): # Assume the ratio of DX(n+1)/DX(n) is the same\n",
    "            MatchedHit_R = {}\n",
    "            closesthit_R = {}\n",
    "            for axis in ['X', 'Y', 'Z']:\n",
    "                MatchedHit_R[axis] = \\\n",
    "                dataDF['MatchedHit_D{}[{}]'.format(axis,station)].dropna().median() / dataDF['MatchedHit_D{}[{}]'.format(axis,station-1)].dropna().median()\n",
    "            for axis in ['X', 'Y', 'Z']:\n",
    "                col = 'MatchedHit_D{}[{}]'.format(axis, station)\n",
    "                col_prev = 'MatchedHit_D{}[{}]'.format(axis, station-1)\n",
    "                ind_null = dataDF[col].isnull()\n",
    "                if ind_null.sum() == 0: \n",
    "                    continue\n",
    "                dataDF.loc[ind_null, col] = dataDF.loc[ind_null, col_prev] * MatchedHit_R[axis]\n",
    "            if not essential:\n",
    "                for axis in ['x', 'y']:\n",
    "                    closesthit_R[axis] = \\\n",
    "                    dataDF['closest_d{}_per_station[{}]'.format(axis,station)].dropna().median() / dataDF['closest_d{}_per_station[{}]'.format(axis,station-1)].dropna().median()\n",
    "                for axis in ['x', 'y']:\n",
    "                    col = 'closest_d{}_per_station[{}]'.format(axis,station)\n",
    "                    col_prev = 'closest_d{}_per_station[{}]'.format(axis,station-1)\n",
    "                    ind_null = dataDF[col].isnull()\n",
    "                    if ind_null.sum() == 0: \n",
    "                        continue\n",
    "                    dataDF.loc[ind_null, col] = dataDF.loc[ind_null, col_prev] * closesthit_R[axis]\n",
    "    \n",
    "    if substitution_f == True:\n",
    "        for mcol in tqdm(imputation_dict.keys(), desc=\"substitution imputation\"):\n",
    "            if mcol not in dataDF.columns:\n",
    "                continue\n",
    "            ind_null = dataDF[mcol].isnull()\n",
    "            if ind_null.sum() == 0: \n",
    "                continue\n",
    "            dataDF.loc[ind_null, mcol] = dataDF.loc[ind_null, imputation_dict[mcol]]\n",
    "    \n",
    "    if mean_f == True:\n",
    "        for col in tqdm(['MatchedHit_Z[2]', 'MatchedHit_Z[3]', 'closest_z_per_station[2]', 'closest_z_per_station[3]',\n",
    "                         'closest_x_per_station[2]', 'closest_x_per_station[3]', 'closest_y_per_station[2]', 'closest_y_per_station[3]', \n",
    "                         'average_x_per_station[2]',  'average_x_per_station[3]',  'average_y_per_station[2]', 'average_y_per_station[3]'], \n",
    "                        desc=\"Mean value imputation\"):\n",
    "            if essential and col in ['average_x_per_station[2]',  'average_x_per_station[3]',  'average_y_per_station[2]', 'average_y_per_station[3]']:\n",
    "                continue\n",
    "            if col not in dataDF.columns:\n",
    "                continue\n",
    "            ind_null = dataDF[col].isnull()\n",
    "            if ind_null.sum() == 0: \n",
    "                continue\n",
    "            dataDF.loc[ind_null, col] = dataDF[col].mean()\n",
    "    \n",
    "    if angle_f == True:\n",
    "        for col in tqdm(['MAngle[0]', 'MAngle[1]', 'MAngle', 'MAngle_v2[0]', 'MAngle_v2[1]', 'MAngle_v2[2]'], \n",
    "                        desc=\"angle imputation\"):\n",
    "            if not col in dataDF.columns:\n",
    "                continue\n",
    "            ind_null = dataDF[col].isnull()\n",
    "            if ind_null.sum() == 0: \n",
    "                continue\n",
    "            vec_col = []\n",
    "            vec_col2 = []\n",
    "            if col == 'MAngle[0]':\n",
    "                vec_col = ['MatchedHit_X[0]','MatchedHit_Y[0]','MatchedHit_Z[0]', \n",
    "                           'MatchedHit_X[1]', 'MatchedHit_Y[1]', 'MatchedHit_Z[1]']\n",
    "                vec_col2 =  ['MatchedHit_X[1]','MatchedHit_Y[1]','MatchedHit_Z[1]', \n",
    "                           'MatchedHit_X[2]', 'MatchedHit_Y[2]', 'MatchedHit_Z[2]']\n",
    "            elif col == 'MAngle[1]':\n",
    "                vec_col =  ['MatchedHit_X[1]','MatchedHit_Y[1]','MatchedHit_Z[1]', \n",
    "                           'MatchedHit_X[2]', 'MatchedHit_Y[2]', 'MatchedHit_Z[2]']\n",
    "                vec_col2 = ['MatchedHit_X[2]','MatchedHit_Y[2]','MatchedHit_Z[2]', \n",
    "                           'MatchedHit_X[3]', 'MatchedHit_Y[3]', 'MatchedHit_Z[3]']\n",
    "            elif col == 'MAngle':\n",
    "                vec_col = ['MatchedHit_X[0]','MatchedHit_Y[0]','MatchedHit_Z[0]', \n",
    "                           'MatchedHit_X[1]', 'MatchedHit_Y[1]', 'MatchedHit_Z[1]']\n",
    "                vec_col2 = [0, 0, 0, 'MatchedHit_X[0]', 'MatchedHit_Y[0]', 'MatchedHit_Z[0]']\n",
    "            elif col == 'MAngle_v2[0]':\n",
    "                vec_col = ['MatchedHit_X[0]','MatchedHit_Y[0]','MatchedHit_Z[0]', \n",
    "                           'MatchedHit_X[1]', 'MatchedHit_Y[1]', 'MatchedHit_Z[1]']\n",
    "                vec_col2 = [0, 0, 0, 'Lextra_X[0]', 'Lextra_Y[0]', 'MatchedHit_Z[0]']\n",
    "            elif col == 'MAngle_v2[1]':\n",
    "                vec_col =  ['MatchedHit_X[1]','MatchedHit_Y[1]','MatchedHit_Z[1]', \n",
    "                           'MatchedHit_X[2]', 'MatchedHit_Y[2]', 'MatchedHit_Z[2]']\n",
    "                vec_col2 = [0, 0, 0, 'Lextra_X[1]', 'Lextra_Y[1]', 'MatchedHit_Z[1]']\n",
    "            elif col == 'MAngle_v2[2]':\n",
    "                vec_col =  ['MatchedHit_X[2]','MatchedHit_Y[2]','MatchedHit_Z[2]', \n",
    "                           'MatchedHit_X[3]', 'MatchedHit_Y[3]', 'MatchedHit_Z[3]']\n",
    "                vec_col2 = [0, 0, 0, 'Lextra_X[2]', 'Lextra_Y[2]', 'MatchedHit_Z[2]']\n",
    "\n",
    "            delta = pd.DataFrame(index = dataDF.index[ind_null])\n",
    "            for i, axis in enumerate(['X', 'Y', 'Z']):\n",
    "                delta.loc[ind_null, axis+'1'] = dataDF.loc[ind_null, vec_col[3+i]] - dataDF.loc[ind_null, vec_col[i]]\n",
    "                if col in ['MAngle[0]','MAngle[1]']:\n",
    "                    delta.loc[ind_null, axis+'2'] = dataDF.loc[ind_null, vec_col2[3+i]] - dataDF.loc[ind_null, vec_col2[i]]\n",
    "                else:\n",
    "                    delta.loc[ind_null, axis+'2'] = dataDF.loc[ind_null, vec_col2[3+i]] - vec_col2[i]\n",
    "            delta = delta[['X1','Y1','Z1','X2','Y2','Z2']]\n",
    "            dataDF.loc[ind_null, col] = delta.progress_apply(lambda x: angle(x.values), axis=1)\n",
    "            del delta\n",
    "    \n",
    "    if average_xy_f == True:\n",
    "        for station in tqdm(range(4), desc='average_xy'):\n",
    "            col = 'closest_xy_per_station[{}]'.format(station)\n",
    "            if not col in dataDF.columns:\n",
    "                continue\n",
    "            ind_null = dataDF[col].isnull()\n",
    "            if ind_null.sum() == 0:\n",
    "                continue\n",
    "            dataDF.loc[ind_null, col] = dataDF.loc[ind_null, 'closest_x_per_station[{}]'.format(station)] + dataDF.loc[ind_null, 'closest_y_per_station[{}]'.format(station)]\n",
    "\n",
    "    gc.collect()\n",
    "    return dataDF\n",
    "\n",
    "def angle(arr):\n",
    "    x = arr[0:3]\n",
    "    y = arr[3:]\n",
    "    dot_xy = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    cos = dot_xy / (norm_x*norm_y)\n",
    "    cos = np.clip(cos, -1, 1)\n",
    "    rad = np.arccos(cos)\n",
    "    theta = rad * 180 / np.pi\n",
    "    return theta\n",
    "\n",
    "def data_check(dataDF):\n",
    "    print(dataDF.shape)\n",
    "    null_col = []\n",
    "    for col in tqdm(dataDF.columns, desc='checking data...,'):\n",
    "        ind_null = dataDF[col].isnull()\n",
    "        if ind_null.sum() == 0:\n",
    "            continue\n",
    "        print('{}: {} null items found'.format(col, ind_null.sum()))\n",
    "        null_col.append(col)\n",
    "        display(dataDF.loc[ind_null, col].head(5))\n",
    "    return null_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataDF):\n",
    "    station = 0\n",
    "    delta = pd.DataFrame(index = dataDF.index)\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        delta[axis+'1'] = dataDF['MatchedHit_{}[{}]'.format(axis, station)]\n",
    "        delta[axis+'2'] = dataDF['MatchedHit_{}[{}]'.format(axis, station + 1)] - dataDF['MatchedHit_{}[{}]'.format(axis, station)]\n",
    "    delta = delta[['X1','Y1','Z1','X2','Y2','Z2']]\n",
    "    dataDF['MAngle'] = delta.progress_apply(lambda x: angle(x.values), axis=1)\n",
    "    \n",
    "    for station in range(3):\n",
    "        delta = pd.DataFrame(index = dataDF.index)\n",
    "        for axis in ['X', 'Y', 'Z']:\n",
    "            delta[axis+'1'] = dataDF['MatchedHit_{}[{}]'.format(axis, station + 1)] - dataDF['MatchedHit_{}[{}]'.format(axis, station)]\n",
    "            delta[axis+'2'] = dataDF['Lextra_{}[{}]'.format(axis, station)] if axis != 'Z' else dataDF['MatchedHit_Z[{}]'.format(station)].mean()\n",
    "        delta = delta[['X1','Y1','Z1','X2','Y2','Z2']]\n",
    "        dataDF['MAngle_v2[{}]'.format(station)] = delta.progress_apply(lambda x: angle(x.values), axis=1)\n",
    "    \n",
    "    del delta\n",
    "    gc.collect()\n",
    "    \n",
    "    for coord in ['X', 'Y']:\n",
    "        for station in range(3):\n",
    "            dataDF['Mextra_D{}2_R[{}]'.format(coord, station)] = \\\n",
    "                dataDF['Mextra_D{}2[{}]'.format(coord, station+1)] / dataDF['Mextra_D{}2[{}]'.format(coord, station)]\n",
    "            \n",
    "            dataDF['MatchedHit_D{}_R[{}]'.format(coord, station)] = \\\n",
    "                dataDF['MatchedHit_D{}[{}]'.format(coord, station+1)] / dataDF['MatchedHit_D{}[{}]'.format(coord, station)]\n",
    "            \n",
    "    for station in range(4):\n",
    "        dataDF['MatchedHit_DXY[{}]'.format(station)] = \\\n",
    "            dataDF['MatchedHit_DX[{}]'.format(station)] + dataDF['MatchedHit_DY[{}]'.format(station)]\n",
    "        \n",
    "        dataDF['Mextra_DXY2[{}]'.format(station)] = \\\n",
    "            dataDF['Mextra_DX2[{}]'.format(station)] + dataDF['Mextra_DY2[{}]'.format(station)]\n",
    "        \n",
    "        dataDF['closest_xy_per_station[{}]'.format(station)] = \\\n",
    "            dataDF['closest_x_per_station[{}]'.format(station)] + dataDF['closest_y_per_station[{}]'.format(station)]\n",
    "    \n",
    "    for station in range(3):\n",
    "        dataDF['MatchedHit_DXY_R[{}]'.format(station)] = \\\n",
    "            dataDF['MatchedHit_DXY[{}]'.format(station+1)] / dataDF['MatchedHit_DXY[{}]'.format(station)]\n",
    "    \n",
    "    dataDF['MAngle[01]_diff'] = dataDF['MAngle[1]'] - dataDF['MAngle[0]']\n",
    "    dataDF['Pratio'] = dataDF['PT'] / dataDF['P']\n",
    "    \n",
    "    return dataDF\n",
    "\n",
    "def feature_engineering_additinal(dataDF):\n",
    "    for coord in ['X', 'Y']:\n",
    "        for station in range(3):\n",
    "            dataDF['MatchedHit_{}_diff[{}]'.format(coord, station)] = \\\n",
    "                dataDF['MatchedHit_{}[{}]'.format(coord, station+1)] - dataDF['MatchedHit_{}[{}]'.format(coord, station)]\n",
    "    \n",
    "    dataDF[\"closest_xy_average\"] = (dataDF[\"closest_xy_per_station[0]\"] + dataDF[\"closest_xy_per_station[1]\"] + dataDF[\"closest_xy_per_station[2]\"] + dataDF[\"closest_xy_per_station[3]\"]) / 4\n",
    "    dataDF[\"closest_x_average\"] = (dataDF[\"closest_x_per_station[0]\"] + dataDF[\"closest_x_per_station[1]\"] + dataDF[\"closest_x_per_station[2]\"] + dataDF[\"closest_x_per_station[3]\"]) / 4\n",
    "    dataDF[\"closest_y_average\"] = (dataDF[\"closest_y_per_station[0]\"] + dataDF[\"closest_y_per_station[1]\"] + dataDF[\"closest_y_per_station[2]\"] + dataDF[\"closest_y_per_station[3]\"]) / 4\n",
    "    \n",
    "    dataDF[\"ncl_average\"] = (dataDF[\"ncl[0]\"] + dataDF[\"ncl[1]\"] + dataDF[\"ncl[2]\"] + dataDF[\"ncl[3]\"]) / 4\n",
    "    dataDF[\"avg_cs_average\"] = (dataDF[\"avg_cs[0]\"] + dataDF[\"avg_cs[1]\"] + dataDF[\"avg_cs[2]\"] + dataDF[\"avg_cs[3]\"]) / 4\n",
    "    \n",
    "    for station in range(4):\n",
    "        dataDF['average_xy_per_station[{}]'.format(station)] = \\\n",
    "            dataDF['average_x_per_station[{}]'.format(station)] + dataDF['average_y_per_station[{}]'.format(station)]\n",
    "\n",
    "    return dataDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = utils.load_hdf('01_rawdata/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trn_data[utils.TRAIN_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train =  make_Xdata(trn_data[utils.SIMPLE_FEATURE_COLUMNS + utils.FOI_COLUMNS + utils.TRAIN_COLUMNS], additional_feature=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = feature_engineering(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[utils.COL_ESSENTIAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = missing_value_imputation(x_train, essential=True, \n",
    "                                   ratio_f=False, mean_f=False, substitution_f=False, angle_f=True, average_xy_f=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = data_check(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trn_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something to do with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_flipper(data, threshold=4000, multiplier=2.0):\n",
    "    ind_negative = (data['weight'] < 0)\n",
    "    \n",
    "    ind_label0_negative = (data[\"weight\"] > -threshold) & (data['weight'] < 0) & (data[\"label\"] == 0)\n",
    "    data.loc[ind_label0_negative, 'weight'] = data['weight'].map(lambda x: multiplier * x)\n",
    "    \n",
    "    data.loc[ind_negative, 'weight'] = data['weight'].map(lambda x: -1.0 * x)\n",
    "    data.loc[ind_negative, 'label'] = data['label'].map(lambda x: 1-x)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_raw = y_train['weight'].copy()\n",
    "y_raw = y_train['label'].copy()\n",
    "flipped_y = weight_flipper(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv_trn, x_cv_test, y_cv_trn, y_cv_test, w_cv_trn, w_cv_test = \\\n",
    "    train_test_split(x_train, flipped_y['label'], flipped_y['weight'], test_size=0.2, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Pool(\n",
    "    data = x_cv_trn.values,\n",
    "    label = y_cv_trn.values,\n",
    "    weight = w_cv_trn.values)\n",
    "eval_data = Pool(\n",
    "    data = x_cv_test.values,\n",
    "    label = y_cv_test.values,\n",
    "    weight = w_cv_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {'iterations':10000, 'eval_metric':'RMSE', 'one_hot_max_size':5,\n",
    "              'use_best_model':True, 'random_state':None, 'thread_count':7}\n",
    "fit_params = {'early_stopping_rounds':10, 'verbose':False, 'plot':True}\n",
    "\n",
    "cat = CatBoostRegressor(depth=7, random_strength=40, bagging_temperature=0.2, \n",
    "                              learning_rate=0.06, \n",
    "                              **cat_params)\n",
    "cat.fit(X=train_data, eval_set=eval_data, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'imp': cat.feature_importances_, 'col': x_cv_trn.columns})\n",
    "_ = importance.plot(kind='barh', x='col', y='imp', figsize=(20, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cat.predict(x_cv_test)\n",
    "scoring.rejection90(y_raw.loc[y_cv_test.index], pred, sample_weight=w_raw.loc[x_cv_test.index].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.save_model('02_models/track2_sota.cbm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
